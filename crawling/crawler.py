#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
í‚¤ì›Œë“œ ê¸°ë°˜ ê²Œì‹œê¸€/ëŒ“ê¸€ ëª¨ë‹ˆí„°ë§
"""

import json
import logging
import multiprocessing
import os
import random
import re
import sys
import time
import traceback
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from urllib.parse import quote

from playwright.sync_api import sync_playwright, Page, Browser, Frame, TimeoutError as PlaywrightTimeoutError
from openpyxl import Workbook
from openpyxl.styles import Font, Alignment
from pydantic import BaseModel, Field, field_validator
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')


# íƒ€ì… ë³„ì¹­
FrameLike = Union[Page, Frame]


@dataclass(frozen=True)
class Timeouts:
    """íƒ€ì„ì•„ì›ƒ ì„¤ì •"""
    PAGE_LOAD = 30000  # 30ì´ˆ
    ELEMENT_WAIT = 5000  # 5ì´ˆ
    COMMENT_LOAD = 3000  # 3ì´ˆ
    NETWORK_IDLE = 10000  # 10ì´ˆ


@dataclass(frozen=True)
class WaitTimes:
    """ëŒ€ê¸° ì‹œê°„ ì„¤ì • (ë°€ë¦¬ì´ˆ)"""
    AFTER_LOGIN = 2000
    AFTER_PAGE_LOAD = 500
    AFTER_IFRAME_SWITCH = 1000
    AFTER_COMMENT_CLICK = 2000
    BETWEEN_PAGES = 1000
    SCROLL_INTERVAL = 300


@dataclass(frozen=True)
class RetryConfig:
    """ì¬ì‹œë„ ì„¤ì •"""
    MAX_ATTEMPTS = 3
    MIN_WAIT = 1
    MAX_WAIT = 10


@dataclass(frozen=True)
class CrawlerConstants:
    """í¬ë¡¤ëŸ¬ ìƒìˆ˜"""
    MAX_SCROLL_ATTEMPTS = 5
    LOGIN_TIMEOUT_SECONDS = 120
    LOGIN_CHECK_INTERVAL = 10


class CafeInfo(BaseModel):
    """ì¹´í˜ ì •ë³´"""
    cafe_id: str = Field(..., pattern=r'^\d+$', description="ì¹´í˜ ID")
    cafe_name: str = Field(..., min_length=1, description="ì¹´í˜ ì´ë¦„")
    cafe_url: str = Field(..., pattern=r'^https?://', description="ì¹´í˜ URL")


class AccountConfig(BaseModel):
    """ê³„ì • ì •ë³´"""
    naver_id: str = Field(..., min_length=1, description="ë„¤ì´ë²„ ì•„ì´ë””")
    naver_password: str = Field(..., min_length=1, description="ë„¤ì´ë²„ ë¹„ë°€ë²ˆí˜¸")
    assigned_cafes: List[str] = Field(..., min_length=1, description="ë‹´ë‹¹ ì¹´í˜ ëª©ë¡")
    group_name: str = Field(..., min_length=1, description="ê·¸ë£¹ ì´ë¦„")


class CrawlerSettings(BaseModel):
    """í¬ë¡¤ëŸ¬ ì„¤ì • (Pydantic ê²€ì¦)"""
    accounts: List[AccountConfig] = Field(..., min_length=1, description="ê³„ì • ëª©ë¡")
    cafes: List[CafeInfo] = Field(..., min_length=1, description="ì¹´í˜ ëª©ë¡")
    keywords: List[str] = Field(..., min_length=1, description="ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡")
    output_folder: str = Field(default='results', description="ì¶œë ¥ í´ë”")
    log_folder: str = Field(default='logs', description="ë¡œê·¸ í´ë”")

    @field_validator('keywords')
    @classmethod
    def validate_keywords(cls, v):
        if not v or len(v) == 0:
            raise ValueError("keywords must have at least one item")
        return v

    @field_validator('cafes')
    @classmethod
    def validate_cafes(cls, v):
        if not v or len(v) == 0:
            raise ValueError("cafes must have at least one item")
        return v


@dataclass(frozen=True)
class Selectors:
    """CSS ì„ íƒì ìƒìˆ˜"""

    # ë¡œê·¸ì¸
    LOGIN_ID: str = '#id'
    LOGIN_PW: str = '#pw'
    LOGIN_BUTTON: str = '#log\\.login'
    LOGIN_BUTTON_ALT: str = 'button[type="submit"]'
    LOGIN_ERROR: str = '.error_msg, .alert_msg, .err'

    # ê²€ìƒ‰ ê²°ê³¼
    ARTICLE_LINKS: tuple = (
        'div.inner_list a.article',
        'a.article',
        'div.inner_list a',
        'a[href*="/articles/"]',
        '.article-board a.article',
        'a[href*="ArticleRead"]'
    )

    # ê²Œì‹œê¸€ ì •ë³´
    TITLE: str = 'h3.title_text, .title_text, .title_area'
    AUTHOR: str = 'button.nickname, .nickname, .nick_name, .article_writer .nickname'
    DATE: str = 'span.date, .date, .article_info .date, .write_date'
    CONTENT: str = '.article_viewer, .se-main-container, .content, #content'
    LIKES: tuple = (
        'em._count',
        'em.u_cnt',
        '._count',
        '.u_cnt',
        '.like_count',
        '.like-count'
    )

    # ëŒ“ê¸€
    COMMENT_BUTTON: tuple = (
        'a.button_comment',
        'a[class*="button_comment"]',
        'button[class*="comment"]',
        'a[href*="comment"]',
        '*[class*="button_comment"]'
    )
    COMMENT_ITEMS: tuple = (
        'ul.comment_list li.CommentItem',
        'li.CommentItem',
        'ul.comment_list li',
        'div.comment_area'
    )
    COMMENT_AUTHOR: str = 'a.comment_nickname'
    COMMENT_TEXT: str = 'span.text_comment'


class NaverCafeCrawler:
    """ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""

    def __init__(self, config_path: str = "config.json", account_info: AccountConfig = None, group_name: str = None):
        """ì´ˆê¸°í™”"""
        # ì„¤ì • ë¡œë“œ ë° ê²€ì¦
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = json.load(f)
        self.config = CrawlerSettings(**config_dict)

        self.account_info = account_info
        self.group_name = group_name

        self.logger = self._setup_logger()
        self.collected_data: List[Dict[str, Any]] = []

        # ë¸Œë¼ìš°ì € ê´€ë ¨
        self.playwright = None
        self.browser: Browser = None
        self.context = None
        self.page: Page = None

        # ìƒìˆ˜
        self.selectors = Selectors()
        self.timeouts = Timeouts()
        self.wait_times = WaitTimes()
        self.constants = CrawlerConstants()

        # íƒ€ì´ë¨¸ ë° ì¬ì‹œì‘ ê´€ë ¨
        self.process_start_time = time.time()
        self.last_restart_time = time.time()
        self.restart_interval = 1800  # 30ë¶„ (ì´ˆ ë‹¨ìœ„) - ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•´ í•„ìš”ì‹œ 1200 (20ë¶„) ë˜ëŠ” 1500 (25ë¶„)ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥
        self.restart_count = 0

        # ì¤‘ë³µ URL ì²´í¬ìš©
        self.existing_urls = set()

    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        log_folder = Path(self.config.log_folder)
        log_folder.mkdir(parents=True, exist_ok=True)

        log_filename = log_folder / f"crawler_{self.group_name}_{datetime.now().strftime('%Y%m%d')}.log"

        logger = logging.getLogger(f'NaverCafeCrawler_{self.group_name}')
        logger.setLevel(logging.INFO)

        if logger.handlers:
            logger.handlers.clear()

        # íŒŒì¼ í•¸ë“¤ëŸ¬ (DEBUG)
        file_handler = logging.FileHandler(log_filename, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)

        # ì½˜ì†” í•¸ë“¤ëŸ¬ (INFO)
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)

        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )

        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)

        logger.addHandler(file_handler)
        logger.addHandler(console_handler)

        return logger

    def _get_date_range(self) -> tuple:
        """íŒŒì¼ëª…ìš© ë‚ ì§œ ë²”ìœ„ ê³„ì‚°"""
        today = datetime.now()
        weekday = today.weekday()
        this_monday = today if weekday == 0 else today - timedelta(days=weekday)
        last_tuesday = this_monday - timedelta(days=6)
        return last_tuesday, this_monday

    def _normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        if not text:
            return ""
        text = text.replace('\n', ' ').replace('\r', ' ')
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _wait(self, milliseconds: int):
        """ë°€ë¦¬ì´ˆ ë‹¨ìœ„ ëŒ€ê¸°"""
        time.sleep(milliseconds / 1000.0)

    def _wait_for_element(self, frame: FrameLike, selector: str, timeout: int = None) -> bool:
        """ìš”ì†Œê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°"""
        try:
            timeout = timeout or self.timeouts.ELEMENT_WAIT
            frame.wait_for_selector(selector, timeout=timeout, state='visible')
            return True
        except PlaywrightTimeoutError:
            return False

    def _find_element_with_selectors(
        self,
        frame: FrameLike,
        selectors: tuple,
        description: str = "ìš”ì†Œ",
        wait: bool = True
    ) -> Optional[Any]:
        """ì—¬ëŸ¬ ì„ íƒìë¡œ ìš”ì†Œ ì°¾ê¸°"""
        for selector in selectors:
            if wait:
                self._wait_for_element(frame, selector, timeout=2000)

            elem = frame.locator(selector).first
            if elem.count() > 0:
                self.logger.debug(f"{description} ë°œê²¬: {selector}")
                return elem
        return None

    def _extract_article_id(self, url: str) -> Optional[str]:
        """URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ"""
        match = re.search(r'/articles/(\d+)', url)
        return match.group(1) if match else None

    def _find_iframe(self, patterns: List[str], frame_name: str = None) -> Optional[Frame]:
        """iframe ì°¾ê¸°"""
        frames = self.page.frames

        for frame in frames:
            # name ìš°ì„  í™•ì¸
            if frame_name:
                try:
                    if frame.name == frame_name:
                        self.logger.debug(f"iframe ë°œê²¬ (name): {frame_name}")
                        return frame
                except:
                    pass

            # URL íŒ¨í„´ í™•ì¸
            frame_url = frame.url
            for pattern in patterns:
                if pattern in frame_url:
                    self.logger.debug(f"iframe ë°œê²¬ (íŒ¨í„´: {pattern})")
                    return frame

        return None

    def _find_matching_inner_iframe(self, parent_frame: Frame, article_id: str) -> Optional[Frame]:
        """ì¤‘ì²© iframe ì¤‘ í˜„ì¬ ê²Œì‹œê¸€ IDì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒ ì°¾ê¸°"""
        inner_frames = parent_frame.child_frames

        if len(inner_frames) == 0:
            return None

        self.logger.debug(f"ë‚´ë¶€ iframe {len(inner_frames)}ê°œ ë°œê²¬, ê²Œì‹œê¸€ ID: {article_id}")

        for inner_frame in inner_frames:
            frame_url = inner_frame.url

            if '/ca-fe/cafes/' in frame_url and '/articles/' in frame_url:
                iframe_article_id = self._extract_article_id(frame_url)

                if iframe_article_id == article_id:
                    self.logger.debug(f"ì¼ì¹˜í•˜ëŠ” iframe ë°œê²¬: {frame_url}")
                    return inner_frame

        return None

    @retry(
        stop=stop_after_attempt(RetryConfig.MAX_ATTEMPTS),
        wait=wait_exponential(min=RetryConfig.MIN_WAIT, max=RetryConfig.MAX_WAIT),
        retry=retry_if_exception_type((PlaywrightTimeoutError, Exception)),
        reraise=True
    )
    def _collect_comments(self, article_frame: FrameLike, url: str) -> List[str]:
        """ëŒ“ê¸€ ìˆ˜ì§‘ (ì¬ì‹œë„ í¬í•¨)"""
        comments = []

        # ëŒ“ê¸€ ë²„íŠ¼ í´ë¦­
        try:
            self.logger.debug("ëŒ“ê¸€ ë²„íŠ¼ í´ë¦­ ì‹œë„")
            comment_button = self._find_element_with_selectors(
                article_frame, self.selectors.COMMENT_BUTTON, "ëŒ“ê¸€ ë²„íŠ¼", wait=True
            )

            if comment_button:
                comment_button.click()
                self._wait(self.wait_times.AFTER_COMMENT_CLICK)
                self.logger.debug("ëŒ“ê¸€ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ")
            else:
                self.logger.debug("ëŒ“ê¸€ ë²„íŠ¼ ì—†ìŒ, ìŠ¤í¬ë¡¤ ì‹œë„")
                for _ in range(self.constants.MAX_SCROLL_ATTEMPTS):
                    article_frame.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                    self._wait(self.wait_times.SCROLL_INTERVAL)
        except Exception as e:
            self.logger.debug(f"ëŒ“ê¸€ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}, ìŠ¤í¬ë¡¤ë¡œ ëŒ€ì²´")
            for _ in range(self.constants.MAX_SCROLL_ATTEMPTS):
                article_frame.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                self._wait(self.wait_times.SCROLL_INTERVAL)

        # ëŒ“ê¸€ ìˆ˜ì§‘
        try:
            # ëŒ“ê¸€ ë¡œë”© ëŒ€ê¸°
            if not self._wait_for_element(article_frame, self.selectors.COMMENT_ITEMS[0], timeout=self.timeouts.COMMENT_LOAD):
                self.logger.debug(f"ëŒ“ê¸€ ì—†ìŒ: {url}")
                return comments

            comment_count = article_frame.locator(self.selectors.COMMENT_ITEMS[0]).count()
            self.logger.debug(f"ëŒ“ê¸€ {comment_count}ê°œ ìˆ˜ì§‘ ì‹œì‘")

            for i in range(comment_count):
                try:
                    comment_elem = article_frame.locator(self.selectors.COMMENT_ITEMS[0]).nth(i)

                    author_elem = comment_elem.locator(self.selectors.COMMENT_AUTHOR).first
                    author = author_elem.inner_text().strip() if author_elem.count() > 0 else "ìµëª…"

                    text_elem = comment_elem.locator(self.selectors.COMMENT_TEXT).first
                    text = self._normalize_text(text_elem.inner_text()) if text_elem.count() > 0 else ""

                    if text:
                        comments.append(f"{author} : {text}")

                except Exception as e:
                    self.logger.debug(f"ëŒ“ê¸€ {i} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    continue

            self.logger.debug(f"ëŒ“ê¸€ {len(comments)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

        except Exception as e:
            self.logger.debug(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return comments

    @contextmanager
    def browser_context(self):
        """ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        try:
            self._start_browser()
            yield self.page
        finally:
            self._close_browser()

    def _start_browser(self):
        """ë¸Œë¼ìš°ì € ì‹œì‘"""
        self.logger.info("ë¸Œë¼ìš°ì € ì‹œì‘")

        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(
            headless=False,
            args=[
                '--start-maximized',
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--disable-gpu',  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì†Œí™”
                '--disable-software-rasterizer',  # ì†Œí”„íŠ¸ì›¨ì–´ ë Œë”ë§ ë¹„í™œì„±í™”
                '--disable-extensions',  # í™•ì¥ í”„ë¡œê·¸ë¨ ë¹„í™œì„±í™”
                '--no-sandbox',  # ìƒŒë“œë°•ìŠ¤ ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
                '--disable-setuid-sandbox',
                '--disable-features=TranslateUI',  # ë²ˆì—­ ê¸°ëŠ¥ ë¹„í™œì„±í™”
                '--disable-features=Translate',
                '--js-flags=--expose-gc'  # JavaScript ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ í™œì„±í™”
            ]
        )

        self.context = self.browser.new_context(
            viewport=None,
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        self.page = self.context.new_page()
        self.logger.info("ë¸Œë¼ìš°ì € ì „ì²´í™”ë©´ìœ¼ë¡œ ì‹œì‘ë¨")

    def _close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ (ë©”ëª¨ë¦¬ ì •ë¦¬ í¬í•¨)"""
        try:
            # í˜ì´ì§€ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if self.page:
                try:
                    self.page.evaluate('() => { window.stop(); }')
                    self.page.evaluate('() => { console.clear(); }')
                except:
                    pass

            # ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬
            if self.context:
                try:
                    self.context.close()
                except:
                    pass

            # ë¸Œë¼ìš°ì € ì¢…ë£Œ
            if self.browser:
                self.browser.close()
                self.logger.info("ë¸Œë¼ìš°ì € ì¢…ë£Œ")

            # Playwright ì •ë¦¬
            if self.playwright:
                self.playwright.stop()

            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            import gc
            gc.collect()

        except Exception as e:
            self.logger.warning(f"ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

    def _get_cookie_path(self) -> Path:
        """ì¿ í‚¤ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        return Path(self.config.log_folder) / f"cookies_{self.group_name}.json"

    def _save_cookies(self):
        """ë¸Œë¼ìš°ì € ì¿ í‚¤ ì €ì¥"""
        try:
            cookies = self.context.cookies()
            cookie_path = self._get_cookie_path()
            with open(cookie_path, 'w', encoding='utf-8') as f:
                json.dump(cookies, f)
            self.logger.info(f"ì¿ í‚¤ ì €ì¥ ì™„ë£Œ: {cookie_path}")
        except Exception as e:
            self.logger.warning(f"ì¿ í‚¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _load_cookies(self) -> bool:
        """ì €ì¥ëœ ì¿ í‚¤ ë¡œë“œ"""
        try:
            cookie_path = self._get_cookie_path()
            if not cookie_path.exists():
                self.logger.info("ì €ì¥ëœ ì¿ í‚¤ ì—†ìŒ")
                return False

            with open(cookie_path, 'r', encoding='utf-8') as f:
                cookies = json.load(f)

            self.context.add_cookies(cookies)
            self.logger.info("ì¿ í‚¤ ë¡œë“œ ì™„ë£Œ")

            # ì¿ í‚¤ ìœ íš¨ì„± í™•ì¸ (ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ ì ‘ì†)
            self.page.goto('https://www.naver.com', wait_until='domcontentloaded')
            self._wait(1000)

            # ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ (ê°„ë‹¨í•œ ì²´í¬)
            page_content = self.page.content()
            if 'nid.naver.com' not in page_content:
                self.logger.info("ì¿ í‚¤ ìœ íš¨ í™•ì¸ë¨")
                return True
            else:
                self.logger.warning("ì¿ í‚¤ ë§Œë£Œë¨")
                return False

        except Exception as e:
            self.logger.warning(f"ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def _load_existing_urls(self):
        """ì—‘ì…€ íŒŒì¼ì—ì„œ ê¸°ì¡´ URL ë¡œë“œ"""
        try:
            output_folder = Path(self.config.output_folder)
            last_tuesday, this_monday = self._get_date_range()
            filename = f"í‚¤ì›Œë“œëª¨ë‹ˆí„°ë§_{self.group_name}_{last_tuesday.strftime('%Y-%m-%d')}~{this_monday.strftime('%d')}.xlsx"
            filepath = output_folder / filename

            if not filepath.exists():
                self.logger.info("ê¸°ì¡´ ì—‘ì…€ íŒŒì¼ ì—†ìŒ, ì²˜ìŒë¶€í„° ì‹œì‘")
                self.existing_urls = set()
                return

            from openpyxl import load_workbook
            wb = load_workbook(filepath, read_only=True)
            ws = wb.active

            # URL ì»¬ëŸ¼ ì°¾ê¸° (í—¤ë” í–‰ì—ì„œ 'URL' ì°¾ê¸°)
            url_col = None
            for col_idx, cell in enumerate(ws[1], 1):
                if cell.value == 'URL':
                    url_col = col_idx
                    break

            if url_col:
                # ëª¨ë“  URL ìˆ˜ì§‘ (í—¤ë” ì œì™¸)
                urls = set()
                for row in ws.iter_rows(min_row=2, min_col=url_col, max_col=url_col):
                    url = row[0].value
                    if url:
                        urls.add(url)

                self.existing_urls = urls
                self.logger.info(f"ê¸°ì¡´ URL {len(urls)}ê°œ ë¡œë“œ ì™„ë£Œ")
            else:
                self.logger.warning("ì—‘ì…€ íŒŒì¼ì— URL ì»¬ëŸ¼ ì—†ìŒ")
                self.existing_urls = set()

            wb.close()

        except Exception as e:
            self.logger.warning(f"ê¸°ì¡´ URL ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.existing_urls = set()

    def _should_restart_browser(self) -> bool:
        """ë¸Œë¼ìš°ì € ì¬ì‹œì‘ í•„ìš” ì—¬ë¶€ í™•ì¸"""
        elapsed = time.time() - self.last_restart_time
        return elapsed >= self.restart_interval

    def _restart_browser_if_needed(self):
        """í•„ìš”ì‹œ ë¸Œë¼ìš°ì € ì¬ì‹œì‘"""
        if not self._should_restart_browser():
            return False

        elapsed_minutes = int((time.time() - self.last_restart_time) / 60)
        self.restart_count += 1

        self.logger.info("="*80)
        self.logger.info(f"ë¸Œë¼ìš°ì € ì¬ì‹œì‘ #{self.restart_count} (ê²½ê³¼: {elapsed_minutes}ë¶„)")
        self.logger.info("="*80)
        print(f"\n{'='*80}")
        print(f"ğŸ”„ [{self.group_name}] ë¸Œë¼ìš°ì € ì¬ì‹œì‘ #{self.restart_count} (ë©”ëª¨ë¦¬ ìµœì í™”)")
        print(f"   ê²½ê³¼ ì‹œê°„: {elapsed_minutes}ë¶„")
        print(f"{'='*80}\n")

        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        self._close_browser()
        self._wait(2000)  # 2ì´ˆ ëŒ€ê¸°

        # ë¸Œë¼ìš°ì € ì¬ì‹œì‘
        self._start_browser()

        # ì¿ í‚¤ ë¡œë“œ ì‹œë„
        cookie_loaded = self._load_cookies()

        if not cookie_loaded:
            # ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì¬ë¡œê·¸ì¸
            self.logger.info("ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨, ì¬ë¡œê·¸ì¸ í•„ìš”")
            print(f"[{self.group_name}] ì¬ë¡œê·¸ì¸ í•„ìš”...\n")
            if not self.login_naver():
                raise Exception("ì¬ë¡œê·¸ì¸ ì‹¤íŒ¨")
            self._save_cookies()
        else:
            self.logger.info("ì¿ í‚¤ë¡œ ë¡œê·¸ì¸ ìƒëµ")
            print(f"[{self.group_name}] ì¿ í‚¤ ì¬ì‚¬ìš© (ë¡œê·¸ì¸ ìƒëµ)\n")

        # ê¸°ì¡´ URL ë‹¤ì‹œ ë¡œë“œ
        self._load_existing_urls()

        # íƒ€ì´ë¨¸ ë¦¬ì…‹
        self.last_restart_time = time.time()

        return True

    def login_naver(self) -> bool:
        """ë„¤ì´ë²„ ë¡œê·¸ì¸"""
        self.logger.info("ë„¤ì´ë²„ ë¡œê·¸ì¸ ì‹œì‘")

        try:
            self.page.goto('https://nid.naver.com/nidlogin.login', wait_until='domcontentloaded')
            self._wait(self.wait_times.AFTER_PAGE_LOAD)

            # ì•„ì´ë””/ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
            self.logger.debug(f"ì•„ì´ë”” ì…ë ¥: {self.account_info.naver_id}")
            self.page.fill(self.selectors.LOGIN_ID, self.account_info.naver_id)
            self._wait(500)

            self.logger.debug("ë¹„ë°€ë²ˆí˜¸ ì…ë ¥")
            self.page.fill(self.selectors.LOGIN_PW, self.account_info.naver_password)
            self._wait(500)

            # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
            self.logger.debug("ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­")
            try:
                login_button = self.page.locator(self.selectors.LOGIN_BUTTON)
                if login_button.count() > 0:
                    login_button.click()
                else:
                    self.page.click(self.selectors.LOGIN_BUTTON_ALT)
            except Exception as e:
                self.logger.debug(f"ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨, Enter í‚¤ ì‚¬ìš©: {e}")
                self.page.press(self.selectors.LOGIN_PW, 'Enter')

            self._wait(self.wait_times.AFTER_LOGIN)

            # ë¡œê·¸ì¸ ì˜¤ë¥˜ í™•ì¸
            try:
                error_msg = self.page.locator(self.selectors.LOGIN_ERROR).first
                if error_msg.count() > 0:
                    error_text = error_msg.inner_text()
                    self.logger.error(f"ë¡œê·¸ì¸ ì˜¤ë¥˜: {error_text}")
                    print(f"\nâŒ ë¡œê·¸ì¸ ì˜¤ë¥˜: {error_text}\n")

                screenshot_path = Path(self.config.log_folder) / f'login_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
                self.page.screenshot(path=str(screenshot_path))
            except Exception as e:
                self.logger.debug(f"ìŠ¤í¬ë¦°ìƒ· ì‹¤íŒ¨: {e}")

            # ë¡œê·¸ì¸ ì™„ë£Œ ëŒ€ê¸°
            self.logger.info("ë¡œê·¸ì¸ ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 120ì´ˆ)")
            print("\n" + "="*80)
            print("âš ï¸  ë¡œê·¸ì¸ ì²˜ë¦¬ ì¤‘...")
            print("    - ë³´ì•ˆì¸ì¦ì´ í•„ìš”í•˜ë©´ ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ì²˜ë¦¬í•´ì£¼ì„¸ìš”")
            print("="*80 + "\n")

            max_checks = self.constants.LOGIN_TIMEOUT_SECONDS // self.constants.LOGIN_CHECK_INTERVAL
            for i in range(max_checks):
                self._wait(self.constants.LOGIN_CHECK_INTERVAL * 1000)
                current_url = self.page.url

                if 'nid.naver.com/nidlogin' not in current_url:
                    elapsed = (i+1) * self.constants.LOGIN_CHECK_INTERVAL
                    self.logger.info(f"ë¡œê·¸ì¸ ì„±ê³µ (ì†Œìš”: {elapsed}ì´ˆ)")
                    print(f"\nâœ… ë¡œê·¸ì¸ ì„±ê³µ (ì†Œìš”: {elapsed}ì´ˆ)\n")
                    
                    # ì¿ í‚¤ ì €ì¥
                    self._save_cookies()
                    
                    return True

                remaining = self.constants.LOGIN_TIMEOUT_SECONDS - (i+1) * self.constants.LOGIN_CHECK_INTERVAL
                print(f"[{(i+1) * self.constants.LOGIN_CHECK_INTERVAL}ì´ˆ] ëŒ€ê¸° ì¤‘... (ë‚¨ì€ ì‹œê°„: {remaining}ì´ˆ)")

            # íƒ€ì„ì•„ì›ƒ
            if 'nid.naver.com/nidlogin' in self.page.url:
                self.logger.error("ë¡œê·¸ì¸ ì‹¤íŒ¨: íƒ€ì„ì•„ì›ƒ")
                print("\nâŒ ë¡œê·¸ì¸ ì‹¤íŒ¨: íƒ€ì„ì•„ì›ƒ\n")
                return False

            return True

        except Exception as e:
            self.logger.error(f"ë¡œê·¸ì¸ ì˜¤ë¥˜: {e}")
            return False

    @retry(
        stop=stop_after_attempt(RetryConfig.MAX_ATTEMPTS),
        wait=wait_exponential(min=RetryConfig.MIN_WAIT, max=RetryConfig.MAX_WAIT),
        retry=retry_if_exception_type((PlaywrightTimeoutError,)),
        reraise=True
    )
    def search_keyword_in_cafe(self, cafe_id: str, keyword: str, page_num: int) -> List[Dict[str, Any]]:
        """ì¹´í˜ ë‚´ í‚¤ì›Œë“œ ê²€ìƒ‰ (ì¬ì‹œë„ í¬í•¨)"""
        self.logger.debug(f"'{keyword}' {page_num}í˜ì´ì§€ ê²€ìƒ‰")

        posts = []

        try:
            encoded_keyword = quote(keyword)
            search_url = f"https://cafe.naver.com/f-e/cafes/{cafe_id}/menus/0?viewType=L&ta=ARTICLE_COMMENT&page={page_num}&q={encoded_keyword}&p=7d"

            self.page.goto(search_url, wait_until='domcontentloaded', timeout=self.timeouts.PAGE_LOAD)
            self._wait(self.wait_times.AFTER_PAGE_LOAD)

            # ê²€ìƒ‰ ê²°ê³¼ iframe ì°¾ê¸°
            search_frame = self._find_iframe(['ArticleSearchList', 'menus'])

            if not search_frame:
                self.logger.debug("ê²€ìƒ‰ iframe ë¯¸ë°œê²¬, ë©”ì¸ í˜ì´ì§€ ì‚¬ìš©")
                search_frame = self.page

            # ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°
            link_elements = self._find_element_with_selectors(
                search_frame, self.selectors.ARTICLE_LINKS, "ê²Œì‹œê¸€ ë§í¬"
            )

            if not link_elements:
                self.logger.debug(f"'{keyword}' {page_num}í˜ì´ì§€ ê²°ê³¼ ì—†ìŒ")
                return posts

            link_count = search_frame.locator(self.selectors.ARTICLE_LINKS[0]).count()
            self.logger.debug(f"{link_count}ê°œ ê²Œì‹œê¸€ ë°œê²¬")

            # URL ìˆ˜ì§‘
            for i in range(link_count):
                try:
                    link = search_frame.locator(self.selectors.ARTICLE_LINKS[0]).nth(i)
                    href = link.get_attribute('href')

                    if href and '/articles/' in href:
                        if href.startswith('http'):
                            post_url = href
                        elif href.startswith('/'):
                            post_url = f"https://cafe.naver.com{href}"
                        else:
                            post_url = f"https://cafe.naver.com/{href}"

                        posts.append({'keyword': keyword, 'url': post_url})

                except Exception as e:
                    self.logger.debug(f"URL ìˆ˜ì§‘ ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {i}): {e}")
                    continue

        except Exception as e:
            self.logger.warning(f"'{keyword}' {page_num}í˜ì´ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            raise

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        try:
            self.page.evaluate('() => { if (window.gc) window.gc(); }')
        except:
            pass

        self.logger.debug(f"'{keyword}' {page_num}í˜ì´ì§€: {len(posts)}ê°œ URL ìˆ˜ì§‘")
        return posts

    @retry(
        stop=stop_after_attempt(RetryConfig.MAX_ATTEMPTS),
        wait=wait_exponential(min=RetryConfig.MIN_WAIT, max=RetryConfig.MAX_WAIT),
        retry=retry_if_exception_type((PlaywrightTimeoutError,)),
        reraise=True
    )
    def collect_post_details(self, post_info: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ì¬ì‹œë„ í¬í•¨)"""
        url = post_info['url']
        keyword = post_info['keyword']
        cafe_name = post_info['cafe_name']

        # ì¤‘ë³µ URL ì²´í¬
        if url in self.existing_urls:
            self.logger.debug(f"ì¤‘ë³µ URL ê±´ë„ˆë›°ê¸°: {url}")
            return None

        try:
            # ê²Œì‹œê¸€ í˜ì´ì§€ ì´ë™
            self.page.goto(url, wait_until='domcontentloaded', timeout=self.timeouts.PAGE_LOAD)
            self._wait(self.wait_times.AFTER_PAGE_LOAD)

            # ì¶”ê°€ ëŒ€ê¸° - ë™ì  ì½˜í…ì¸  ë¡œë”©ì„ ìœ„í•´
            self._wait(2000)

            # ëª¨ë“  í”„ë ˆì„ì—ì„œ ìš”ì†Œ ì°¾ê¸° ì‹œë„
            article_frame = None
            for frame in self.page.frames:
                try:
                    # ì œëª©ì´ ìˆëŠ” í”„ë ˆì„ì„ ì°¾ìŒ
                    if frame.locator('h3.title_text').count() > 0:
                        article_frame = frame
                        self.logger.debug(f"ê²Œì‹œê¸€ í”„ë ˆì„ ë°œê²¬: {frame.url}")
                        break
                except:
                    continue

            if not article_frame:
                self.logger.warning(f"ê²Œì‹œê¸€ í”„ë ˆì„ ë¯¸ë°œê²¬, ë©”ì¸ í˜ì´ì§€ ì‚¬ìš© ({url})")
                self.logger.debug(f"ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë ˆì„ ìˆ˜: {len(self.page.frames)}")
                for idx, frame in enumerate(self.page.frames):
                    self.logger.debug(f"í”„ë ˆì„ {idx}: {frame.url}")
                article_frame = self.page

            # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            try:
                # ì œëª©
                title_elem = article_frame.locator(self.selectors.TITLE).first
                title = self._normalize_text(title_elem.inner_text()) if title_elem.count() > 0 else ""
                self.logger.debug(f"ì œëª©: {title[:50] if title else 'None'}...")

                # ì‘ì„±ì
                author_elem = article_frame.locator(self.selectors.AUTHOR).first
                author = author_elem.inner_text().strip() if author_elem.count() > 0 else ""
                self.logger.debug(f"ì‘ì„±ì: {author if author else 'None'}")

                # ë‚ ì§œ
                date_elem = article_frame.locator(self.selectors.DATE).first
                date_str = date_elem.inner_text().strip() if date_elem.count() > 0 else ""
                self.logger.debug(f"ë‚ ì§œ: {date_str if date_str else 'None'}")

                # ë³¸ë¬¸ ë‚´ìš©
                content_elem = article_frame.locator(self.selectors.CONTENT).first
                content = self._normalize_text(content_elem.inner_text()) if content_elem.count() > 0 else ""
                self.logger.debug(f"ë‚´ìš© ê¸¸ì´: {len(content)} ê¸€ì")

                # ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
                like_elem = self._find_element_with_selectors(article_frame, self.selectors.LIKES, "ì¢‹ì•„ìš”", wait=False)
                likes = "0"
                if like_elem and like_elem.count() > 0:
                    likes_text = like_elem.inner_text().strip()
                    likes = re.sub(r'\D', '', likes_text) if likes_text else "0"
                    if not likes:  # ìˆ«ìê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ
                        likes = "0"
                self.logger.debug(f"ì¢‹ì•„ìš”: {likes}")

            except Exception as e:
                self.logger.error(f"ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")
                self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                return None

            # ëŒ“ê¸€ ìˆ˜ì§‘ (ì¤‘ì²© iframe ì²˜ë¦¬ ì œê±° - Frame detached ì˜¤ë¥˜ ë°©ì§€)
            comments = []
            try:
                comments = self._collect_comments(article_frame, url)
            except Exception as comment_err:
                self.logger.debug(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì‹¤íŒ¨: {comment_err}")

            if len(comments) == 0:
                self.logger.debug(f"ëŒ“ê¸€ ì—†ìŒ: {url}")
            else:
                self.logger.debug(f"ëŒ“ê¸€ {len(comments)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

            # ìˆ˜ì§‘ ì™„ë£Œ í›„ existing_urlsì— ì¶”ê°€
            self.existing_urls.add(url)

            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë” ì ê·¹ì )
            try:
                # JavaScript ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
                self.page.evaluate('() => { if (window.gc) window.gc(); }')
                # í˜ì´ì§€ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
                self.page.evaluate('() => { window.stop(); }')
                # ì½˜ì†” ë¡œê·¸ ì •ë¦¬
                self.page.evaluate('() => { console.clear(); }')
            except:
                pass

            return {
                'ì±„ë„': cafe_name,
                'í‚¤ì›Œë“œ': keyword,
                'ë‹‰ë„¤ì„': author,
                'ë‚ ì§œ': date_str,
                'ì œëª©': title,
                'ë‚´ìš©': content,
                'ì¢‹ì•„ìš”': likes,
                'URL': url,
                'ëŒ“ê¸€': comments
            }

        except Exception as e:
            self.logger.warning(f"ê²Œì‹œê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜ ({url}): {e}")
            raise

    def _save_batch_to_excel(self, keyword: str):
        """í‚¤ì›Œë“œë³„ ë°°ì¹˜ ì €ì¥"""
        if len(self.collected_data) == 0:
            self.logger.info(f"'{keyword}' ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ, ì €ì¥ ê±´ë„ˆëœ€")
            return

        self.logger.info(f"'{keyword}' {len(self.collected_data)}ê°œ ë°ì´í„° ì €ì¥ ì¤‘...")

        # ì¶œë ¥ í´ë” ìƒì„±
        output_folder = Path(self.config.output_folder)
        output_folder.mkdir(parents=True, exist_ok=True)

        # íŒŒì¼ëª… ìƒì„±
        last_tuesday, this_monday = self._get_date_range()
        filename = f"í‚¤ì›Œë“œëª¨ë‹ˆí„°ë§_{self.group_name}_{last_tuesday.strftime('%Y-%m-%d')}~{this_monday.strftime('%d')}.xlsx"
        filepath = output_folder / filename

        # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if filepath.exists():
            from openpyxl import load_workbook
            wb = load_workbook(filepath)
            ws = wb.active
            existing_rows = ws.max_row
        else:
            wb = Workbook()
            ws = wb.active
            ws.title = "í‚¤ì›Œë“œëª¨ë‹ˆí„°ë§"
            existing_rows = 0

            # í—¤ë” ìƒì„± (ìƒˆ íŒŒì¼ì¸ ê²½ìš°)
            max_comments = max((len(data.get('ëŒ“ê¸€', [])) for data in self.collected_data), default=0)
            headers = ['ì±„ë„', 'í‚¤ì›Œë“œ', 'ë‹‰ë„¤ì„', 'ë‚ ì§œ', 'ì œëª©', 'ë‚´ìš©', 'ì¢‹ì•„ìš”', 'URL']
            headers.extend([f'ëŒ“ê¸€{i}' for i in range(1, max_comments + 1)])

            ws.append(headers)
            for cell in ws[1]:
                cell.font = Font(bold=True)
                cell.alignment = Alignment(horizontal='center', vertical='center')

        # í˜„ì¬ ìµœëŒ€ ëŒ“ê¸€ ì»¬ëŸ¼ ìˆ˜ í™•ì¸
        current_max_cols = ws.max_column
        new_max_comments = max((len(data.get('ëŒ“ê¸€', [])) for data in self.collected_data), default=0)
        required_cols = 8 + new_max_comments  # ê¸°ë³¸ 8ê°œ + ëŒ“ê¸€ ì»¬ëŸ¼

        # í•„ìš”ì‹œ ëŒ“ê¸€ ì»¬ëŸ¼ ì¶”ê°€
        if required_cols > current_max_cols:
            for i in range(current_max_cols - 7, new_max_comments + 1):
                ws.cell(row=1, column=8 + i, value=f'ëŒ“ê¸€{i}')
                ws.cell(row=1, column=8 + i).font = Font(bold=True)
                ws.cell(row=1, column=8 + i).alignment = Alignment(horizontal='center', vertical='center')

        # ë°ì´í„° ì¶”ê°€
        for data in self.collected_data:
            row = [
                data.get('ì±„ë„', ''),
                data.get('í‚¤ì›Œë“œ', ''),
                data.get('ë‹‰ë„¤ì„', ''),
                data.get('ë‚ ì§œ', ''),
                data.get('ì œëª©', ''),
                data.get('ë‚´ìš©', ''),
                data.get('ì¢‹ì•„ìš”', '0'),
                data.get('URL', '')
            ]

            comments = data.get('ëŒ“ê¸€', [])
            row.extend(comments)

            # ë‚¨ì€ ì»¬ëŸ¼ ë¹ˆì¹¸
            max_cols = ws.max_column
            row.extend([''] * (max_cols - len(row)))

            ws.append(row)

        # ì»¬ëŸ¼ ë„ˆë¹„ ì¡°ì •
        for column in ws.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    cell_length = len(str(cell.value))
                    if cell_length > max_length:
                        max_length = cell_length
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws.column_dimensions[column_letter].width = adjusted_width

        # ì €ì¥
        wb.save(filepath)
        self.logger.info(f"'{keyword}' ì €ì¥ ì™„ë£Œ: {filepath}")
        print(f"  ğŸ’¾ '{keyword}' {len(self.collected_data)}ê°œ ì €ì¥: {filepath}")

        # ë©”ëª¨ë¦¬ í•´ì œ
        self.collected_data.clear()

    def _crawl_keyword(self, cafe_id: str, cafe_name: str, keyword: str, keyword_idx: int, total_keywords: int):
        """ë‹¨ì¼ í‚¤ì›Œë“œ í¬ë¡¤ë§"""
        print(f"\n{'='*80}")
        print(f"[{cafe_name}] [í‚¤ì›Œë“œ {keyword_idx}/{total_keywords}] '{keyword}' ê²€ìƒ‰ ì‹œì‘")
        print(f"{'='*80}")
        self.logger.info(f"[{cafe_name}] [{keyword_idx}/{total_keywords}] '{keyword}' ì‹œì‘")

        page_num = 1
        keyword_total_posts = 0

        while True:
            print(f"\n[{cafe_name}] [{keyword}] {page_num}í˜ì´ì§€ ê²€ìƒ‰ ì¤‘...")

            try:
                # ê²Œì‹œê¸€ URL ìˆ˜ì§‘
                posts = self.search_keyword_in_cafe(cafe_id, keyword, page_num)

                # ê° postì— cafe_name ì¶”ê°€
                for post in posts:
                    post['cafe_name'] = cafe_name

                if len(posts) == 0:
                    print(f"  â†’ {page_num}í˜ì´ì§€ ê²Œì‹œê¸€ ì—†ìŒ. '{keyword}' ì¢…ë£Œ")
                    self.logger.info(f"'{keyword}' {page_num}í˜ì´ì§€ ì—†ìŒ, ì¢…ë£Œ")
                    break

                print(f"  â†’ {len(posts)}ê°œ ë°œê²¬")
                self.logger.info(f"'{keyword}' {page_num}í˜ì´ì§€: {len(posts)}ê°œ")

                # ê° ê²Œì‹œê¸€ ì²˜ë¦¬
                page_collected = 0
                for post_idx, post_info in enumerate(posts, 1):
                    print(f"  [{post_idx}/{len(posts)}] ì²˜ë¦¬ ì¤‘...")

                    # ê²Œì‹œê¸€ ì²˜ë¦¬ ì „ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì²´í¬ (ë” ìì£¼ ì²´í¬)
                    try:
                        restarted = self._restart_browser_if_needed()
                        if restarted:
                            self.logger.info(f"ë¸Œë¼ìš°ì € ì¬ì‹œì‘ í›„ ê²Œì‹œê¸€ ì²˜ë¦¬ ê³„ì†")
                    except Exception as e:
                        self.logger.error(f"ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì‹¤íŒ¨: {e}")
                        raise

                    try:
                        post_data = self.collect_post_details(post_info)

                        if post_data:
                            self.collected_data.append(post_data)
                            page_collected += 1
                            keyword_total_posts += 1
                            comment_count = len(post_data.get('ëŒ“ê¸€', []))
                            print(f"    âœ… ì™„ë£Œ (ëŒ“ê¸€ {comment_count}ê°œ)")
                        else:
                            print(f"    â­ï¸  ìˆ˜ì§‘ ì‹¤íŒ¨, ê±´ë„ˆëœ€")

                    except Exception as e:
                        self.logger.warning(f"ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨ ({post_info['url']}): {e}")
                        print(f"    âŒ ì˜¤ë¥˜, ê±´ë„ˆëœ€")
                        continue

                print(f"\n  {page_num}í˜ì´ì§€ ì™„ë£Œ: {page_collected}ê°œ")

                # ë‹¤ìŒ í˜ì´ì§€ë¡œ
                page_num += 1

                # ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì²´í¬
                try:
                    restarted = self._restart_browser_if_needed()
                    if restarted:
                        self.logger.info(f"ë¸Œë¼ìš°ì € ì¬ì‹œì‘ í›„ '{keyword}' ê³„ì†")
                except Exception as e:
                    self.logger.error(f"ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì‹¤íŒ¨: {e}")
                    raise

                # Rate limiting: ëœë¤ ëŒ€ê¸° ì‹œê°„ ì¶”ê°€
                random_wait = random.uniform(500, 2000)
                self._wait(self.wait_times.BETWEEN_PAGES + random_wait)

            except Exception as e:
                self.logger.error(f"'{keyword}' {page_num}í˜ì´ì§€ ì˜¤ë¥˜: {e}")
                print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ, ë‹¤ìŒ í‚¤ì›Œë“œë¡œ")
                break

        print(f"\n'{keyword}' ì™„ë£Œ: ì´ {keyword_total_posts}ê°œ")
        self.logger.info(f"'{keyword}' ì™„ë£Œ: {keyword_total_posts}ê°œ")

        # í‚¤ì›Œë“œë³„ ë°°ì¹˜ ì €ì¥
        self._save_batch_to_excel(keyword)

    def _setup(self):
        """ì´ˆê¸° ì„¤ì • (ë¡œê·¸ì¸ë§Œ)"""
        self.logger.info("="*80)
        self.logger.info("ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ ì‹œì‘")
        self.logger.info("="*80)

        # ê¸°ì¡´ URL ë¡œë“œ
        self._load_existing_urls()

        # ë¡œê·¸ì¸
        if not self.login_naver():
            raise Exception("ë¡œê·¸ì¸ ì‹¤íŒ¨")

    def _crawl_cafe(self, cafe: CafeInfo, cafe_idx: int, total_cafes: int):
        """ë‹¨ì¼ ì¹´í˜ í¬ë¡¤ë§"""
        print(f"\n{'='*80}")
        print(f"[ì¹´í˜ {cafe_idx}/{total_cafes}] {cafe.cafe_name} í¬ë¡¤ë§ ì‹œì‘")
        print(f"{'='*80}")
        self.logger.info(f"[ì¹´í˜ {cafe_idx}/{total_cafes}] {cafe.cafe_name} ì‹œì‘")

        # ëª¨ë“  í‚¤ì›Œë“œ í¬ë¡¤ë§
        keywords = self.config.keywords
        total_keywords = len(keywords)

        for keyword_idx, keyword in enumerate(keywords, 1):
            try:
                self._crawl_keyword(cafe.cafe_id, cafe.cafe_name, keyword, keyword_idx, total_keywords)
            except Exception as e:
                self.logger.error(f"[{cafe.cafe_name}] '{keyword}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                print(f"\nâŒ [{cafe.cafe_name}] '{keyword}' í¬ë¡¤ë§ ì‹¤íŒ¨, ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ì´ë™\n")
                continue

        print(f"\n{'='*80}")
        print(f"[ì¹´í˜ {cafe_idx}/{total_cafes}] {cafe.cafe_name} ì™„ë£Œ")
        print(f"{'='*80}")
        self.logger.info(f"[ì¹´í˜ {cafe_idx}/{total_cafes}] {cafe.cafe_name} ì™„ë£Œ")

    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        try:
            with self.browser_context():
                self._setup()

                # ë‹´ë‹¹ ì¹´í˜ë§Œ í•„í„°ë§
                assigned_cafe_names = self.account_info.assigned_cafes
                cafes = [cafe for cafe in self.config.cafes if cafe.cafe_name in assigned_cafe_names]
                total_cafes = len(cafes)

                for cafe_idx, cafe in enumerate(cafes, 1):
                    try:
                        self._crawl_cafe(cafe, cafe_idx, total_cafes)
                    except Exception as e:
                        self.logger.error(f"ì¹´í˜ í¬ë¡¤ë§ ì‹¤íŒ¨ ({cafe.cafe_name}): {e}")
                        print(f"\nâŒ ì¹´í˜ í¬ë¡¤ë§ ì‹¤íŒ¨ ({cafe.cafe_name}), ë‹¤ìŒ ì¹´í˜ë¡œ ì´ë™\n")
                        continue

            self.logger.info("="*80)
            self.logger.info("í¬ë¡¤ë§ ì™„ë£Œ!")
            self.logger.info("="*80)
            print(f"\n{'='*80}")
            print("âœ… ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"{'='*80}\n")

        except Exception as e:
            self.logger.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            self.logger.error(traceback.format_exc())
            print(f"\nâŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}\n")


def run_crawler_for_account(account_info_dict: dict, config_path: str):
    """ê° ê³„ì •ë³„ë¡œ í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ í•¨ìˆ˜"""
    try:
        account_info = AccountConfig(**account_info_dict)
        group_name = account_info.group_name

        print(f"\n[{group_name}] í”„ë¡œì„¸ìŠ¤ ì‹œì‘ (ê³„ì •: {account_info.naver_id})")
        print(f"[{group_name}] ë‹´ë‹¹ ì¹´í˜: {', '.join(account_info.assigned_cafes)}\n")

        crawler = NaverCafeCrawler(
            config_path=config_path,
            account_info=account_info,
            group_name=group_name
        )
        crawler.run()

        print(f"\n[{group_name}] í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ\n")
    except Exception as e:
        print(f"\nâŒ [{group_name}] í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜: {e}\n")
        traceback.print_exc()


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ë©€í‹°í”„ë¡œì„¸ì‹± ì‹¤í–‰"""
    print("="*80)
    print("ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ëŸ¬ (ë©€í‹° ë¸Œë¼ìš°ì € ë²„ì „)")
    print("í‚¤ì›Œë“œ ê¸°ë°˜ ê²Œì‹œê¸€/ëŒ“ê¸€ ëª¨ë‹ˆí„°ë§")
    print("="*80)
    print()

    try:
        # ì„¤ì • ë¡œë“œ
        with open("config.json", 'r', encoding='utf-8') as f:
            config_dict = json.load(f)

        config = CrawlerSettings(**config_dict)

        # ê³„ì •ë³„ë¡œ í”„ë¡œì„¸ìŠ¤ ìƒì„±
        processes = []
        for account in config.accounts:
            account_dict = account.model_dump()
            p = multiprocessing.Process(
                target=run_crawler_for_account,
                args=(account_dict, "config.json")
            )
            processes.append(p)
            p.start()
            print(f"âœ… [{account.group_name}] í”„ë¡œì„¸ìŠ¤ ì‹œì‘ë¨ (PID: {p.pid})")

        print(f"\nì´ {len(processes)}ê°œ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘...\n")

        # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
        for p in processes:
            p.join()

        print("\n" + "="*80)
        print("âœ… ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ!")
        print("="*80)

    except Exception as e:
        print(f"\nâŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\n")
        traceback.print_exc()
        return

    print("\ní”„ë¡œê·¸ë¨ ì¢…ë£Œ")


if __name__ == "__main__":
    # Windowsì—ì„œ multiprocessing ì‚¬ìš© ì‹œ í•„ìš”
    multiprocessing.freeze_support()
    main()
